---
layout: post
title:  "Machine Learning"
date:   2018-05-06 12:00:00
categories: machine-learning
---

* TOC
{:toc}

# Introduction to Machine Learning

## What Is Machine Learning

A field of computer science that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel (1959)

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. - Tom Mitchell (1998)

## Supervised vs Unsupervised

Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning feedback available to a learning system:

* Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.

    - Regression: modeling the relationship between inputs and outputs.

    - Classification: inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes.

* Unsupervised learning: No outputs are given to the learning algorithm, leaving it on its own to find structure in its inputs.

    - Clustering: a set of inputs is to be divided into groups. The groups are not known beforehand.

# Supervised Learning

To establish notation for future use, we’ll use $x^{(i)}$ to denote the "input" variables, also called input **features**, and $y^{(i)}$ to denote the "output" or **target** variable that we are trying to predict. A pair $(x^{(i)}, y^{(i)})$ is called a **training example**, and the dataset that we’ll be using to learn - a list of $m$ training examples {$(x^{(i)}, y^{(i)}); i = 1, ... ,m$} - is called a **training set**. We will also use $X$ denote the space of input values, and $Y$ the space of output values.

To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function $h : X \to Y$ so that $h(x)$ is a "good" predictor for the corresponding value of $y$. This function $h$ is called a **hypothesis**.

When the target variable that we’re trying to predict is continuous, we call the learning problem a **regression** problem. When $y$ can take on only a small number of discrete values, we call it a **classification** problem.

## Linear Regression

To perform linear regression, we must decide how we’re going to represent functions/hypotheses $h$ in a computer. As an initial choice, lets say we decide to approximate $y$ as a linear function of $x$:

$$ h_{θ}(x) = θ_0 + θ_1 x_1 + θ_2 x_2 + \enspace ... \enspace + θ_n x_n $$

Here, the $θ_i$ 's are the **parameters** (also called **weights**) parameterizing the space of linear functions mapping from $X$ to $Y$. To simplify our notation, we also introduce the convention of letting $x_0 = 1$ (this is the **intercept term**), so that

$$ h_{θ}(x) = \sum_{i=0}^{n} θ_i x_i = θ^T x $$

where on the right-hand side above we are viewing $θ$ and $x$ both as vectors, and here $n$ is the number of input variables (not counting $x_0$).

Now, given a training set, how do we pick, or learn, the parameters $θ$ ? One reasonable method seems to be to make $h(x)$ close to $y$, at least for the training examples we have. To formalize this, we will define a function that measures, for each value of the $θ$ 's, how close the $h_{θ}(x^{(i)})$ 's are to the corresponding $y^{(i)}$ 's. We define the **cost function**:

$$ J(θ) = \frac{1}{2} \sum_{i=1}^{m} (h_{θ}(x^{(i)}) - y^{(i)})^2 $$

### Least Mean Squares

We want to choose $θ$ so as to minimize $J(θ)$. To do so, lets use a search algorithm that starts with some "initial guess" for $θ$, and that repeatedly changes $θ$ to make $J(θ)$ smaller, until hopefully we converge to a value of $θ$ that minimizes $J(θ)$. Specifically, lets consider the **gradient descent** algorithm, which starts with some initial $θ$, and repeatedly performs the update:

$$ θ_j = θ_j - \alpha \frac{\partial}{\partial θ_j} J(θ)  $$

(This update is simultaneously performed for all values of $j = 0, . . . , n$.)

Here, $\alpha$ is called the **learning rate**. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of $J$.

$$ \frac{\partial}{\partial θ_j} J(θ) = \sum_{i=1}^{m} (h_{θ}(x^{(i)}) - y^{(i)}) x_j^{(i)} $$

Repeat until convergence {

$\quad θ_j = θ_j + \alpha \displaystyle\sum_{i=1}^{m} (y^{(i)} - h_{θ}(x^{(i)})) x_j^{(i)} \enspace (for \enspace every \enspace j)$

}

The rule is called the **LMS** update rule (LMS stands for "least mean squares"), and is also known as the **Widrow-Hoff** learning rule, the magnitude of the update is proportional to the **error term** $(y^{(i)} − h_θ (x^{(i)}))$.

This method looks at every example in the entire training set on every step, and is called **batch gradient descent**. Note that, while gradient descent can be a local minima in general, the optimization problem for linear regression has only one global(assuming the learning rate $\alpha$ is not too large), and no other local, optima; Indeed, $J$ is a convex quadratic function.

the contours of a quadratic function and the trajectory taken by batch gradient descent

![contours](/images/machine-learning/contours.png)

There is an alternative to batch gradient descent that also works very well. Consider the following algorithm:

Loop for i=1 to m {

$\quad θ_j = θ_j + \alpha (y^{(i)} - h_{θ}(x^{(i)})) x_j^{(i)} \enspace (for \enspace every \enspace j)$

}

In this algorithm, we repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called **stochastic gradient descent** (also **incremental gradient descent**). Whereas batch gradient descent has to scan through the entire training set before taking a single step - a costly operation if $m$ is large - stochastic gradient descent can start making progress right away, and continues to make progress with each example it looks at. Often, stochastic gradient descent gets $θ$ "close" to the minimum much faster than batch gradient descent. For these reasons, particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.

Note however that stochastic gradient descent may never "converge" to the minimum, and the parameters $θ$ will keep oscillating around the minimum of $J(θ)$; but in practice most of the values near the minimum will be reasonably good approximations to the true minimum. While it is more common to run stochastic gradient descent as we have described it and with a fixed learning rate $\alpha$, by slowly letting the learning rate $\alpha$ decrease to zero as the algorithm runs, it is also possible to ensure that the parameters will converge to the global minimum rather then merely oscillate around the minimum.

### Least Squares

Gradient descent gives one way of minimizing $J$. Lets discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize $J$ by explicitly taking its derivatives with respect to the $θ_j$ 's, and setting them to zero.

Giving a training set, define the **design matrix** $X$ to be the $m×n$ matrix, actually $m×(n + 1)$ if we include the intercept term, that contains the training examples input values in its rows; Also, let $Y$ be the $m$-dimensional vector containing all the target values from the training set;

Then

$$ J(θ) = \frac{1}{2} \sum_{i=1}^{m} (h_{θ}(x^{(i)}) - y^{(i)})^2 = \frac{1}{2} ||Xθ-Y||_2^2 = \frac{1}{2} (Xθ-Y)^T (Xθ-Y) $$

$$ \nabla_θ J(θ) = X^T X θ − X^T Y $$

To minimize $J$, we set its derivatives to zero, and obtain the **normal equations**:

$$ X^T X θ = X^T Y $$

Thus, the value of $θ$ that minimizes $J(θ)$ is given in closed form by the equation

$$ θ = (X^T X)^{−1} X^T Y $$

<div hidden>_</div>

### Probabilistic Interpretation

Let us assume that the target variables and the inputs are related via the equation

$$ y^{(i)} = θ^T x^{(i)} + \epsilon^{(i)} $$

where $\epsilon^{(i)}$ is an error term that captures either unmodeled effects, or random noise. Let us further assume that the $\epsilon^{(i)}$ are distributed IID (independently and identically distributed) according to a Gaussian distribution with mean zero and variance $σ^2$. We can write this assumption as $\epsilon^{(i)} ∼ N(0, σ^2)$. I.e., the density of $\epsilon^{(i)}$ is given by

$$ p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi} σ} exp \bigg(-\frac{(\epsilon^{(i)})^2}{2σ^2}\bigg) $$

Then

$$ p(y^{(i)}|x^{(i)};θ) = \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{(y^{(i)} - θ^T x^{(i)})^2}{2σ^2}\bigg) $$

<div>
The notation $p(y^{(i)}|x^{(i)};θ)$ indicates that this is the distribution of $y^{(i)}$ given $x^{(i)}$ and parameterized by $θ$. We can also write the distribution of $y^{(i)}$ as $(y^{(i)}|x^{(i)};θ) ∼ N(θ^T x^{(i)}, σ^2)$
</div>

Given the design matrix $X$ and outputs $Y$, then the **likelihood** function of $θ$

$$ L(θ) = L(θ;X,Y) = P(Y|X;θ) = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};θ) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{(y^{(i)} - θ^T x^{(i)})^2}{2σ^2}\bigg) $$

The principal of **maximum likelihood** says that we should choose $θ$ so as to make the data as high probability as possible. I.e., we should choose $θ$ to maximize $L(θ)$. Instead of maximizing $L(θ)$, we maximize the **log likelihood** $\ell(θ)$:

$$ \ell(θ) = log L(θ) = log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{(y^{(i)} - θ^T x^{(i)})^2}{2σ^2}\bigg) \\ = \sum_{i=1}^{m} log \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{(y^{(i)} - θ^T x^{(i)})^2}{2σ^2}\bigg) \\ = m log \frac{1}{\sqrt{2\pi} σ} - \frac{1}{σ^2} \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - θ^T x^{(i)})^2 $$

Hence, maximizing $\ell(θ)$ gives the same answer as minimizing

$$ \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - θ^T x^{(i)})^2 $$

which we recognize to be $J(θ)$, our original least-squares cost function.

### Locally Weighted Linear Regression

![LWR](/images/machine-learning/LWR.png)

The choice of features is important to ensuring good performance of a learning algorithm. The **locally weighted linear regression (LWR)** algorithm which, assuming there is sufficient training data, makes the choice of features less critical.

The locally weighted linear regression algorithm does the following:

* Fit $θ$ to minimize $\displaystyle\sum_{i=1}^{m} w^{(i)} (y^{(i)} - θ^T x^{(i)})^2$
* Output $θ^T x$

Here, the $w^{(i)}$ ’s are non-negative valued **weights**. Intuitively, if $w^{(i)}$ is large for a particular value of $i$, then in picking $θ$, we’ll try hard to make $(y^{(i)} − θ^T x^{(i)})^2$ small. If $w^{(i)}$ is small, then the $(y^{(i)} − θ^T x^{(i)})^2$ error term will be pretty much ignored in the fit.

A fairly standard choice for the weights is

$$ w^{(i)} = exp\bigg(-\frac{(x^{(i)} − x)^2}{2 \tau^2}\bigg) $$

If $x$ is vector-valued, this is generalized for an appropriate choice of $\tau$ or $\sum$.

$$ w^{(i)} = exp\bigg(−\frac{(x^{(i)} − x)^T (x^{(i)} − x)}{2 \tau^2}\bigg) \quad or \quad w^{(i)} = exp\bigg(-\frac{(x^{(i)} − x)^T \sum^{−1} (x^{(i)} − x)}{2}\bigg) $$

<div>
Note that the weights depend on the particular point $x$ at which we’re trying to evaluate $x$. Moreover, if $|x^{(i)} - x|$ is small, then $w^{(i)}$ is close to $1$; and if $|x^{(i)} - x|$ is large, then $w^{(i)}$ is close to $0$. Hence, $θ$ is chosen giving a much higher weight to the errors on training examples close to the query point $x$. The parameter $\tau$ controls how quickly the weight of a training example falls off with distance of its $x^{(i)}$ from the query point $x$. $\tau$ is called the <b>bandwidth parameter</b>.
</div>

Locally weighted linear regression is a **non-parametric** algorithm. The linear regression algorithm that we saw earlier is known as a **parametric** learning algorithm, because it has a fixed, finite number of parameters $θ_i$ 's, which are fit to the data. Once we’ve fit the $θ_i$ 's and stored them away, we no longer need to keep the training data around to make future predictions. In contrast, to make predictions using locally weighted linear regression, we need to keep the entire training set around. The term non-parametric roughly refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis $h$ grows linearly with the size of the training set.

## Classification and Logistic Regression

Classification problem is just like the regression problem, except that the values $y$ we now want to predict take on only a small number of discrete values. For now, we will focus on the **binary classification** problem in which $y$ can take on only two values, $0$ and $1$. $0$ is also called the **negative class**, and $1$ the **positive class**, and they are sometimes also denoted by the symbols "-" and "+". Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the **label** for the training example.

### Logistic Regression

In logistic regression we will choose

$$ h_θ(x) = g(θ^T x) = \frac{1}{1 + e^{-θ^T x}} \quad where \quad g(z) = \frac{1}{1 + e^{-z}} \quad with \quad z = θ^T x $$

$g(z)$ is called the **logistic function** or the **sigmoid function**. The plot of $g(z)$:

![logistic function](/images/machine-learning/logistic-function.png)

$g(z)$ tends towards $1$ as $z \to +\infty$, and $g(z)$ tends towards $0$ as $z \to −\infty$. $g(z)$ is always bounded between $0$ and $1$.

a useful property of the derivative of the sigmoid function

$$ g'(z) = g(z)(1 - g(z)) $$

assume that

$$ \begin{align*}
P(y=1|x;θ) &= h_θ(x) \\
P(y=0|x;θ) &= 1 - h_θ(x)
\end{align*} $$

so that

$$ P(y|x;θ) = (h_θ(x))^y (1 - h_θ(x))^{1-y} $$

so the likelihood of the parameters as

$$ \begin{align*}
L(θ) &= P(Y|X;θ) \\
     &= \prod_{i=1}^{m} P(y^{(i)}|x^{(i)};θ) \\
     &= \prod_{i=1}^{m} (h_θ(x^{(i)}))^{y^{(i)}} (1 - h_θ(x^{(i)}))^{1-y^{(i)}}
\end{align*} $$

to maximize the log likelihood

$$ \begin{align*}
\ell (θ) &= log \text{ } L(θ) \\
         &= \sum_{i=1}^{m} ( y^{(i)} log \text{ } h_θ(x^{(i)}) + (1 - y^{(i)}) log(1 - h_θ(x^{(i)})) )
\end{align*} $$

we can use **gradient ascent**:

$$ θ = θ + \alpha \nabla_θ \ell (θ) $$

take derivatives to derive the stochastic gradient ascent rule:

$$ \begin{align*}
\frac{\partial}{\partial θ_j} \ell(θ) &= \bigg(y \frac{1}{g(θ^T x)} - (1 - y) \frac{1}{1 - g(θ^T x)}\bigg) \frac{\partial}{\partial θ_j} g(θ^T x) \\
&= \bigg(y \frac{1}{g(θ^T x)} - (1 - y) \frac{1}{1 - g(θ^T x)}\bigg) g(θ^T x) (1 - g(θ^T x)) \frac{\partial}{\partial θ_j} θ^T x \\
&= (y(1 - g(θ^T x)) - (1 - y)g(θ^T x)) x_j \\
&= (y - h_θ(x)) x_j
\end{align*} $$

so that:

$$ θ_j = θ_j + \alpha (y^{(i)} - h_θ(x^{(i)})) x_j^{(i)} $$

### Perceptron Learning Algorithm

Output values that are either $0$ or $1$ or exactly, change the definition of $g$ to be the threshold function:

$$
  g(z) =
  \begin{cases}
    1 & \text{if } z \geq 0 \\
    0 & \text{if } z < 0
  \end{cases}
$$

let $h_θ (x) = g(θ^T x)$ and use the update rule

$$ θ_j = θ_j + \alpha (y^{(i)} - h_θ(x^{(i)})) x_j^{(i)} $$

then we have the **perceptron learning algorithm**.

Note however that even though the perceptron may be cosmetically similar to the other algorithms we talked about, it is actually a very different type of algorithm than logistic regression and least squares linear regression; in particular, it is difficult to endow the perceptron’s predictions with meaningful probabilistic interpretations, or derive the perceptron as a maximum likelihood estimation algorithm.

### Newton Method For Maximizing Log Likelihood

**Newton method** for finding a zero of a function. Specifically, suppose we have some function $f : R \to R$, and we wish to find a value of $θ$ so that $f(θ) = 0$. Here, $θ ∈ R$ is a real number. Newton method performs the following update:

$$ θ = θ - \frac{f(θ)}{f'(θ)} $$

Approximating the function $f$ via a linear function that is tangent to $f$ at the current guess $θ$, solving for where that linear function equals to zero, and letting the next guess for $θ$ be where that linear function is zero.

Picture of the Newton method in action:

![newton method](/images/machine-learning/newton-method.png)

The maxima of $\ell$ correspond to points where its derivative $\ell'(θ)$ is zero. So, by letting $f(θ) = \ell'(θ)$, we can use the same algorithm to maximize $\ell$, and we obtain update rule:

$$ θ = θ - \frac{\ell'(θ)}{\ell''(θ)} $$

In logistic regression setting, $θ$ is vector-valued, so we need to generalize Newton method to this setting. The generalization of Newton method to this multidimensional setting also called the **Newton-Raphson method** is given by

$$ θ = θ - H^{-1} \nabla_θ \ell(θ) $$

Here, $\nabla_θ \ell(θ)$ is, as usual, the vector of partial derivatives of $\ell(θ)$ with respect to the $θ_i$ 's; and $H$ is a $(n+1)×(n+1)$ matrix that include the intercept term called the **Hessian**, whose entries are given by

$$ H_{ij} = \frac{\partial^2 \ell(θ)}{\partial θ_i \partial θ_j} $$

Newton-Raphson method typically faster convergence than batch gradient descent, and requires many fewer iterations to get very close to the minimum. One iteration of Newton-Raphson method can, however, be more expensive than one iteration of batch gradient descent, since it requires finding and inverting a $(n+1)×(n+1)$ Hessian matrix; but so long as $n$ is not too large, it is usually much faster overall. When Newton-Raphson method is applied to maximize the logistic regression log likelihood function $\ell(θ)$, the resulting method is also called **Fisher scoring**.

## Generalized Linear Models

<div>
In the regression, we had $(y^{(i)}|x^{(i)};θ) ∼ N(μ, σ^2)$, and in the classification, we had $(y^{(i)}|x^{(i)};θ) ∼ Bernoulli(\phi)$, the definitions of $μ$ and $\phi$ as functions of $x$ and $θ$. Both of these methods are special cases of a broader family of models, called <b>Generalized Linear Models (GLMs)</b>.
</div>

### The Exponential Family

To defining exponential family distributions. We say that a class of distributions is in the exponential family if it can be written in the form

$$ p(y;η) = b(y) exp(η^T T(y) - a(η)) $$

$η$ is called the **natural parameter** (also called the **canonical parameter**) of the distribution; $T(y)$ is the **sufficient statistic** (for the distributions we consider, it will often be the case that $T(y) = y$); and $a(η)$ is the **log partition function**. The quantity $e^{-a(η)}$ essentially plays the role of a normalization constant, that makes sure the distribution $p(y;η)$ sums/integrates over $y$ to $1$.

A fixed choice of $T$, $a$ and $b$ defines a family (or set) of distributions that is parameterized by $η$; as we vary $η$, we then get different distributions within this family.

We now show that the Bernoulli and the Gaussian distributions are exponential family distributions.

<div>
The Bernoulli distribution with mean $\phi$, written $Bernoulli(\phi)$, specifies a distribution over $y ∈ \{0, 1\}$, so that $P(y = 1; \phi) = \phi; P(y = 0; \phi) = 1 − \phi$. As we varying $\phi$, we obtain Bernoulli distributions with different means. We now show that this class of Bernoulli distributions, ones obtained by varying $\phi$, is in the exponential family; i.e., that there is a choice of $T$, $a$ and $b$ so that the exponential family becomes exactly the class of Bernoulli distributions.
</div>

Write the Bernoulli distribution as:

$$ \begin{align*}
P(y;\phi) &= \phi^y (1 - \phi)^{1 - y} \\
          &= exp(y log\phi + (1 - y) log(1 - \phi)) \\
          &= exp((log(\frac{\phi}{1 - \phi}))y + log(1 - \phi))
\end{align*} $$

Thus, the natural parameter is given by $η^T = η = log(\phi/(1 − \phi))$. Invert this definition for $η$ by solving for $\phi$ in terms of $η$, we obtain $\phi = 1/(1 + e^{-η})$. To complete the formulation of the Bernoulli distribution as an exponential family distribution, we also have

$$ \begin{align*}
T(y) &= y \\
a(η) &= -log(1 - \phi) \\
     &= log(1 + e^η) \\
b(y) &= 1
\end{align*} $$

The Gaussian distribution $N(μ, σ^2)$ with mean $μ$ and variance $σ^2$:

$$ \begin{align*}
p(y;η) = p(y;μ,σ) &= \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{(y - μ)^2}{2σ^2}\bigg) \\
                  &= \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{y^2}{2σ^2}\bigg) exp\bigg(-\frac{μ^2-2μy}{2σ^2}\bigg) \\
                  &= \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{y^2}{2σ^2}\bigg) exp\bigg(\frac{μ}{σ}\frac{y}{σ} - \frac{μ^2}{2σ^2}\bigg)
\end{align*} $$

so the Gaussian is in the exponential family, with

$$ \begin{align*}
η^T = η &= \frac{μ}{σ} \quad invert \quad μ = ση \\
   T(y) &= \frac{y}{σ} \\
   a(η) &= \frac{μ^2}{2σ^2} = \frac{η^2}{2} \\
   b(y) &= \frac{1}{\sqrt{2\pi} σ} exp\bigg(-\frac{y^2}{2σ^2}\bigg)
\end{align*} $$

### Constructing GLMs

Make the following assumptions/design choices about the conditional distribution of $y$ given $x$:

<div>
<ul>
<li>
$y | x; θ ∼ ExponentialFamily(η)$. I.e., given $x$ and $θ$, the distribution of $y$ follows some exponential family distribution, with parameter $η$.
</li>

<li>
Given $x$, our goal is to predict the expected value of $T(y)$. So the prediction $h(x)$ output by our learned hypothesis $h$ to satisfy $h(x) = E[T(y)|x]$.
</li>

<li>
The natural parameter $η$ and the inputs $x$ are related linearly: $η = θ^T x$. Or, if $η$ is vector-valued, then $η_i = θ_i^T x$.
</li>
</ul>
</div>

#### Constructing GLMs - Ordinary Least Squares

To show that ordinary least squares is a special case of the GLM family of models, consider the setting where the target variable $y$ (also called the **response variable** in GLM terminology) is continuous, and we model the conditional distribution of $y$ given $x$ as a Gaussian $N(μ, σ^2)$. So, we let the $ExponentialFamily(η)$ distribution be the Gaussian distribution. In the formulation of the Gaussian as an exponential family distribution, $μ/σ = η$. So:

$$ \begin{align*}
h_θ (x) &= E[T(y)|x;θ] = E[\frac{y}{σ}|x;θ] \\
        &= \frac{u}{σ} = η = θ^T x
\end{align*} $$

#### Constructing GLMs - Logistic Regression

<div>
Consider logistic regression. Here we are interested in binary classification, so $y ∈ \{0, 1\}$. Given that $y$ is binary-valued, it therefore seems natural to choose the Bernoulli family of distributions to model the conditional distribution of $y$ given $x$. In our formulation of the Bernoulli distribution as an exponential family distribution, we had $\phi = 1/(1 + e^{-η})$. Furthermore, note that if $(y|x;θ) ∼ Bernoulli(\phi)$, then $E[T(y)|x;θ] = E[y|x;θ] = \phi$. So:
</div>

$$ \begin{align*}
h_θ (x) &= E[T(y)|x;θ] = E[y|x;θ] \\
        &= \phi \\
        &= \frac{1}{1 + e^{-η}} \\
        &= \frac{1}{1 + e^{-θ^T x}}
\end{align*} $$

To introduce a little more terminology, the function $g$ giving the distribution’s mean as a function of the natural parameter $g(η) = E[T(y); η]$ is called the **canonical response function**. Its inverse, $g^{-1}$, is called the **canonical link function**. Thus, the canonical response function for the Gaussian family is just the identify function $g(η) = η$; and the canonical response function for the Bernoulli is the logistic function $g(η) = 1/(1 + e^{-η})$.

#### Constructing GLMs - Softmax Regression

<div>
Consider a classification problem in which the response variable $y$ can take on any one of $k$ values, so $y ∈ \{1, 2, ... , k\}$. We will model it as distributed according to a multinomial distribution.
</div>

Derive a GLM for modelling multinomial data. To do so, Begin by expressing the multinomial as an exponential family distribution.

To parameterize a multinomial over $k$ possible outcomes, use $k$ parameters $\phi_1 , . . . , \phi_k$ specifying the probability of each of the outcomes. However, these parameters would be redundant, or more formally, they would not be independent (since knowing any $k − 1$ of the $\phi_i$ ’s uniquely determines the last one, as they must satisfy $\sum_{i=1}^{k} \phi_i = 1$). So, we will instead parameterize the multinomial with only $k − 1$ parameters, $\phi_1 , . . . , \phi_{k−1}$, where $\phi_i = P(y = i)$, and $P(y = k) = 1 − \sum_{i=1}^{k-1} \phi_i$. For notational convenience, we will also let $\phi_k = 1 − \sum_{i=1}^{k-1} \phi_i$, but we should keep in mind that this is not a parameter, and that it is fully specified by $\phi_1 , . . . , \phi_{k−1}$.

To express the multinomial as an exponential family distribution, we will define $T(y) ∈ R^{k−1}$ as follows:

$$
T(1) =
\begin{bmatrix}
  1 \\
  0 \\
  0 \\
  \vdots \\
  0
\end{bmatrix}, \enspace
T(2) =
\begin{bmatrix}
  0 \\
  1 \\
  0 \\
  \vdots \\
  0
\end{bmatrix}, \enspace
T(3) =
\begin{bmatrix}
  0 \\
  0 \\
  1 \\
  \vdots \\
  0
\end{bmatrix}, ... , \enspace
T(k-1) =
\begin{bmatrix}
  0 \\
  0 \\
  0 \\
  \vdots \\
  1
\end{bmatrix}, \enspace
T(k) =
\begin{bmatrix}
  0 \\
  0 \\
  0 \\
  \vdots \\
  0
\end{bmatrix}
$$

<div>
$T(y)$ is a $k - 1$ dimensional vector, rather than a real number. We will write $(T(y))_i$ to denote the $i$-th element of the vector $T(y)$. So, we can also write the relationship between $T(y)$ and $y$ as $(T(y))_i = 1\{y=i\}$. Further, we have that $E[(T(y))_i] = E[1\{y=i\}] = P(y = i) = \phi_i$.
</div>

<div hidden>_</div>

We are now ready to show that the multinomial is a member of the exponential family. We have:

$$ \begin{align*}
P(y;\phi_1,...,\phi_{k−1}) &= \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} ... \phi_k^{1\{y=k\}} \\
        &= \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} ... \phi_k^{1 - \sum_{i=1}^{k-1} 1\{y=i\}} \\
        &= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} ... \phi_k^{1 - \sum_{i=1}^{k-1} (T(y))_i} \\
        &= exp( (T(y))_1 log(\phi_1) + (T(y))_2 log(\phi_2) + ... + (1 - \sum_{i=1}^{k-1} (T(y))_i) log(\phi_k) ) \\
        &= exp( (T(y))_1 log({\phi_1}/{\phi_k}) + (T(y))_2 log({\phi_2}/{\phi_k}) + ... + (T(y))_{k-1} log({\phi_{k-1}}/{\phi_k}) + log(\phi_k) ) \\
        &= b(y)exp(η^T T(y) - a(η))
\end{align*} $$

where

$$ \begin{align*}
η &=
\begin{bmatrix}
  log(\phi_1/\phi_k) \\
  log(\phi_2/\phi_k) \\
  \vdots \\
  log(\phi_{k-1}/\phi_k)
\end{bmatrix} \\
\\
T(y) &=
\begin{bmatrix}
  1\{y=1\} \\
  1\{y=2\} \\
  \vdots \\
  1\{y=k-1\}
\end{bmatrix} \\
\\
a(η) &= -log(\phi_k) \\
\\
b(y) &= 1
\end{align*} $$

This completes our formulation of the multinomial as an exponential family distribution.

The canonical link function is given (for $i = 1, . . . , k$) by

$$ η_i = log \frac{\phi_i}{\phi_k} $$

For convenience, we have also defined $η_k = log(\phi_k/\phi_k) = 0$. To invert the canonical link function and derive the canonical response function, we therefore have that

$$ \begin{align*}
e^{η_i} &= \frac{\phi_i}{\phi_k} \\
\\
\phi_i &= \phi_k e^{η_i}\\
\\
\sum_{i=1}^{k} \phi_i &= \phi_k \sum_{i=1}^{k} e^{η_i} = 1 \\
\\
\phi_k &= \frac{1}{\sum_{i=1}^{k} e^{η_i}} \\
\\
\phi_i &= \frac{e^{η_i}}{\sum_{j=1}^{k} e^{η_j}}
\end{align*} $$

This function mapping from the $η$ 's to the $\phi$ 's is called the **softmax** function.

To complete our model, so, have $η_i = θ_i^T x$ (for $i = 1, . . . , k − 1$), where $θ_1 , . . . , θ_{k−1} ∈ R^{n+1}$ are the parameters of our model. For notational convenience, we can also define $θ_k = 0$, so that $η_k = θ_k^T x = 0$, as given previously. Hence, our model assumes that the conditional distribution of $y$ given $x$ is given by

$$ \begin{align*}
P(y = i|x; θ) &= \phi_i \\
              \\
              &= \frac{e^{η_i}}{\sum_{j=1}^{k} e^{η_j}} \\
              \\
              &= \frac{e^{θ_i^T x}}{\sum_{j=1}^{k} e^{θ_j^T x}} \\
\end{align*} $$

<div>
This model, which applies to classification problems where $y ∈ \{1, . . . , k\}$, is called <b>softmax regression</b>. It is a generalization of logistic regression.
</div>

Our hypothesis will output

$$ \begin{align*}
h_θ(x) &= E[T(y)|x;θ] \\
       \\
       &= E
       \left[
       \begin{array}{c|c}
         1\{y=1\} \\
         1\{y=2\} \\
         \vdots & x;θ \\
         1\{y=k-1\} \\
       \end{array}
       \right] \\
       \\
       &=
       \begin{bmatrix}
         \phi_1 \\
         \phi_2 \\
         \vdots \\
         \phi_{k-1} \\
       \end{bmatrix} \\
       \\
       &=
       \begin{bmatrix}
         \frac{exp(θ_1^T x)}{\sum_{j=1}^{k} exp(θ_j^T x)} \\
         \frac{exp(θ_2^T x)}{\sum_{j=1}^{k} exp(θ_j^T x)} \\
         \vdots \\
         \frac{exp(θ_{k-1}^T x)}{\sum_{j=1}^{k} exp(θ_j^T x)} \\
       \end{bmatrix} \\
\end{align*} $$

<div>
In other words, our hypothesis will output the estimated probability that $P(y = i|x; θ)$, for every value of $i = 1,...,k$. (Even though $h_θ (x)$ as defined above is only $k − 1$ dimensional, clearly $P(y = k|x; θ)$ can be obtained as $1 - \sum_{i=1}^{k−1} \phi_i$.)
</div>
<p/>
<div>
Lastly, lets discuss parameter fitting. Similar to our original derivation of ordinary least squares and logistic regression, if we have a training set of $m$ examples $\{(x^{(i)}, y^{(i)}); i = 1,...,m\}$ and would like to learn the parameters $θ_i$ of this model, we would begin by writing down the log-likelihood
</div>

$$ \begin{align*}
\ell (θ) &= \sum_{i=1}^{m} log P(y^{(i)}|x^{(i)}, θ) \\
         &= \sum_{i=1}^{m} log \prod_{l=1}^{k} \bigg(\frac{e^{θ_l^T x^{(i)}}}{\sum_{j=1}^{k} e^{θ_j^T x^{(i)}}}\bigg)^{1\{y^{(i)}=l\}}
\end{align*} $$

We can now obtain the maximum likelihood estimate of the parameters by maximizing $\ell (θ)$ in terms of $θ$, using a method such as gradient ascent or Newton method.

## Generative Learning Algorithms

<div>
Algorithms that try to learn $p(y|x)$ directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs $X$ to the labels $\{0, 1\}$, (such as the perceptron algorithm) are called <b>discriminative learning algorithms</b>. Here, we’ll talk about algorithms that instead try to model $p(x|y)$ and $p(y)$. These algorithms are called <b>generative learning algorithms</b>.
</div>
<p/>
<div>
After modeling $p(y)$ (called the <b>class priors</b>) and $p(x|y)$, our algorithm can then use Bayes rule to derive the posterior distribution on $y$ given $x$:
</div>

$$ p(y|x) = \frac{p(x|y)p(y)}{p(x)} $$

<div>
Here, the denominator can also be expressed in terms of the quantities $p(x|y)$ and $p(y)$ that we’ve learned. Actually, if were calculating $p(y|x)$ in order to make a prediction, then we don’t actually need to calculate the denominator, since
</div>

$$ \mathop{argmax}\limits_{y} \enspace p(y|x) = \mathop{argmax}\limits_{y} \enspace \frac{p(x|y)p(y)}{p(x)} = \mathop{argmax}\limits_{y} \enspace p(x|y)p(y) $$

### Gaussian Discriminant Analysis

<div>
In Gaussian discriminant analysis (GDA), we’ll assume that $p(x|y)$ is distributed according to a multivariate normal distribution. Lets talk briefly about the properties of multivariate normal distributions before moving on to the GDA model itself.
</div>

#### The Multivariate Normal Distribution

The multivariate normal distribution in $n$-dimensions, also called the multivariate Gaussian distribution, is parameterized by a **mean vector** $μ ∈ R^n$ and a **covariance matrix** $Σ ∈ R^{n×n}$, where $Σ ≥ 0$ is symmetric and positive semi-definite. Also written $N (μ, Σ)$, its density is given by:

$$ p(x; μ, Σ) = \frac{1}{(2π)^{n/2} |Σ|^{1/2}} exp(-\frac{1}{2} (x − μ)^T Σ^{-1} (x − μ)) $$

For a vector-valued random variable $X$ distributed $N(μ, Σ)$, the mean is given by $μ$:

$$ E[X] = \int_x x \text{ } p(x; μ, Σ) \text{ } dx = μ $$

The **covariance** of a vector-valued random variable $Z$ is defined as $Cov(Z) = E[(Z − E[Z])(Z − E[Z])^T ]$. This generalizes the notion of the variance of a real-valued random variable. The covariance can also be defined as $Cov(Z) = E[ZZ^T] − (E[Z])(E[Z])^T$. If $X ∼ N (μ, Σ)$, then

$$ Cov(X) = Σ $$

#### The Gaussian Discriminant Analysis Model

<div>
When we have a classification problem in which the input features $x$ are continuous-valued random variables, we can then use the Gaussian Discriminant Analysis (GDA) model, which models $p(x|y)$ using a multivariate normal distribution. The model is:
</div>

$$ \begin{align*}
y &∼ Bernoulli(\phi) \\
(x|y = 0) &∼ N (μ_0, Σ) \\
(x|y = 1) &∼ N (μ_1, Σ)
\end{align*} $$

Writing out the distributions, this is:

$$ \begin{align*}
P(y) &= \phi^{y} (1 − \phi)^{1−y} \\
p(x|y = 0) &= \frac{1}{(2π)^{n/2} |Σ|^{1/2}} exp(-\frac{1}{2} (x − μ_0)^T Σ^{-1} (x − μ_0)) \\
p(x|y = 1) &= \frac{1}{(2π)^{n/2} |Σ|^{1/2}} exp(-\frac{1}{2} (x − μ_1)^T Σ^{-1} (x − μ_1))
\end{align*} $$

Here, the parameters of our model are $\phi, Σ, μ_0$ and $μ_1$. (Note that while there’re two different mean vectors $μ_0$ and $μ_1$, this model is usually applied using only one covariance matrix $Σ$.) The log-join-likelihood of the data is given by

$$ \begin{align*}
\ell (\phi, μ_0 , μ_1 , Σ) &= log \prod_{i=1}^{m} p(x^{(i)}, y^{(i)};\phi, μ_0 , μ_1 , Σ) \\
                           &= log \prod_{i=1}^{m} p(x^{(i)}|y^{(i)};μ_0 , μ_1 , Σ) p(y^{(i)};\phi)
\end{align*} $$

By maximizing $\ell$ with respect to the parameters, we find the maximum likelihood estimate of the parameters to be:

$$ \begin{align*}
\phi &= \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)}=1\} \\
\\
μ_0 &= \frac{\sum_{i=1}^{m} 1\{y^{(i)}=0\} x^{(i)}}{\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \\
\\
μ_1 &= \frac{\sum_{i=1}^{m} 1\{y^{(i)}=1\} x^{(i)}}{\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \\
\\
Σ &= \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - μ_{y^{(i)}}) (x^{(i)} - μ_{y^{(i)}})^T
\end{align*} $$

Pictorially, what the algorithm is doing can be seen in as follows:

![gda](/images/machine-learning/gda.png)

<div>
Shown in the figure are the training set, as well as the contours of the two Gaussian distributions that have been fit to the data in each of the two classes. Note that the two Gaussians have contours that are the same shape and orientation, since they share a covariance matrix $Σ$, but they have different means $μ_0$ and $μ_1$. Also shown in the figure is the straight line giving the decision boundary at which $p(y = 1|x) = 0.5$. On one side of the boundary, we’ll predict $y = 1$ to be the most likely outcome, and on the other side, we’ll predict $y = 0$.
</div>

#### Discussion: GDA and Logistic Regression

<div>
The GDA model has an interesting relationship to logistic regression. If we view the quantity $P(y = 1|x; \phi, μ_0 , μ_1 , Σ)$ as a function of $x$, we’ll find that it can be expressed in the form of logistic regression
</div>

$$ P(y = 1|x; \phi, μ_0 , μ_1 , Σ) = \frac{1}{1 + exp(-θ^T x)} $$

because:

$$ \begin{align*}
p(y=1|x) &= \frac{p(x|y=1) p(y=1)}{p(x)} \\
         &= \frac{p(x|y=1) p(y=1)}{p(x|y=0) p(y=0) + p(x|y=1) p(y=1)} \\
         &= \frac{1}{1 + \frac{p(x|y=0) p(y=0)}{p(x|y=1) p(y=1)}}
\end{align*} $$

then:

$$ \begin{align*}
& \frac{p(x|y=0) p(y=0)}{p(x|y=1) p(y=1)} \\
&= exp\bigg(-\frac{1}{2} (x − μ_0)^T Σ^{-1} (x − μ_0) + \frac{1}{2} (x − μ_1)^T Σ^{-1} (x − μ_1)\bigg) \times \frac{1 - \phi}{\phi} \\
&= exp\bigg(\frac{1}{2} (x^T Σ^{-1} μ_0 − x^T Σ^{-1} μ_1 +  μ_0^T Σ^{-1} x − μ_1^T Σ^{-1} x − μ_0^T Σ^{-1} μ_0 + μ_1^T Σ^{-1} μ_1)\bigg) \times exp\bigg(\log(\frac{1-\phi}{\phi})\bigg) \\
&= exp\bigg((μ_0 - μ_1)^T Σ^{-1} x - \frac{μ_0^T Σ^{-1} μ_0 - μ_1^T Σ^{-1} μ_1}{2} +\log(\frac{1-\phi}{\phi})\bigg) \\
&= exp\bigg((μ_0 - μ_1)^T Σ^{-1} x + \Big(\log(\frac{1-\phi}{\phi}) - \frac{μ_0^T Σ^{-1} μ_0 - μ_1^T Σ^{-1} μ_1}{2}\Big) x_0 \bigg) \\
\end{align*} $$

so:

$$ θ = -
\left[
\begin{array}{c}
  \log(\frac{1-\phi}{\phi}) - \frac{μ_0^T Σ^{-1} μ_0 - μ_1^T Σ^{-1} μ_1}{2} \\
  Σ^{-1} (μ_0 - μ_1) \\
\end{array}
\right]
$$

<div>
where $θ$ is some appropriate function of $\phi, μ_0 , μ_1 , Σ$. (uses the convention of redefining the $x^{(i)}$ ’s to be $n + 1$-dimensional vectors by adding the extra coordinate $x_0^{(i)} = 1$) This is exactly the form that logistic regression - a discriminative algorithm - used to model $P(y = 1|x)$.
</div>

When would we prefer one model over another? GDA and logistic regression will, in general, give different decision boundaries when trained on the same dataset. Which is better?

<div>
We just argued that if $p(x|y)$ is multivariate gaussian (with shared $Σ$), then $P(y|x)$ necessarily follows a logistic function. The converse, however, is not true; i.e., $P(y|x)$ being a logistic function does not imply $p(x|y)$ is multivariate gaussian. This shows that GDA makes stronger modeling assumptions about the data than does logistic regression. It turns out that when these modeling assumptions are correct, then GDA will find better fits to the data, and is a better model. Specifically, when $p(x|y)$ is indeed gaussian (with shared $Σ$), then GDA is <b>asymptotically efficient</b>. Informally, this means that in the limit of very large training sets (large $m$), there is no algorithm that is strictly better than GDA (in terms of, say, how accurately they estimate $P(y|x)$). In particular, it can be shown that in this setting, GDA will be a better algorithm than logistic regression; and more generally, even for small training set sizes, we would generally expect GDA to better.
</div>
<p/>
<div>
In contrast, by making significantly weaker assumptions, logistic regression is also more robust and less sensitive to incorrect modeling assumptions. There are many different sets of assumptions that would lead to $P(y|x)$ taking the form of a logistic function. For example, if $(x|y = 0) ∼ Poisson(λ_0)$, and $(x|y = 1) ∼ Poisson(λ_1)$, then $P(y|x)$ will be logistic. Logistic regression will also work well on Poisson data like this. But if we were to use GDA on such data - and fit Gaussian distributions to such non-Gaussian data - then the results will be less predictable, and GDA may or may not do well.
</div>
<p/>
<div>
Turns out if you assume $p(x|y=1) ∼ ExponentialFamily(η_1)$ and $p(x|y=0) ∼ ExponentialFamily(η_0)$, then this implies that $P(y = 1|x)$ is also logistic. So the same exponential family distribution for the two classes with different natural parameters then the posterior $P(y = 1|x)$ would be logistic, and so this shows the robustness of logistic regression to the choice of modeling assumptions because it could be that the data can be anyone of exponential family distribution. So it’s the robustness of logistic regression to modeling assumptions.
</div>

To summarize: GDA makes stronger modeling assumptions, and is more data efficient (i.e., requires less training data to learn “well”) when the modeling assumptions are correct or at least approximately correct. Logistic regression makes weaker assumptions, and is significantly more robust to deviations from modeling assumptions. Specifically, when the data is indeed non-Gaussian, then in the limit of large datasets, logistic regression will almost always do better than GDA. For this reason, in practice logistic regression is used more often than GDA.

### Naive Bayes

#### Naive Bayes

In GDA, the feature vectors $x$ were continuous, real-valued vectors. Lets now talk about a different learning algorithm to be used in **text classification** in which the $x_i$ ’s are discrete-valued.

We will represent the text via a feature vector whose length is equal to the number of words in the dictionary. Specifically, if the text contains the $i$-th word of the dictionary, then we will set $x_i = 1$; otherwise, we let $x_i = 0$.

For instance, the vector

$$
x =
\left[
\begin{array}{c}
  1 \\
  0 \\
  0 \\
  \vdots \\
  1 \\
  \vdots \\
  0 \\
\end{array}
\right]
\begin{array}{c}
  a \\
  coat \\
  cold \\
  \vdots \\
  buy \\
  \vdots \\
  zoo \\
\end{array}
$$

is used to represent the text that contains the words “a” and “buy,” but not “coat”, “cold” or “zoo”. The set of words encoded into the feature vector is called the **vocabulary**, so the dimension of $x$ is equal to the size of the vocabulary.

Actually, rather than looking through an english dictionary for the list of all english words, in practice it is more common to look through our training set and encode in our feature vector only the words that occur at least once there. Apart from reducing the number of words modeled and hence reducing our computational and space requirements, this also has the advantage of allowing us to model/include as a feature many words that may appear in the text but that you won’t find in a dictionary. Sometimes, we also exclude the very high frequency words (which will be words like “the”, “of”, “and”; these high frequency, “content free” words are called **stop words**) since they occur in so many documents and do little to indicate the meaning of the text.

<div>
Having chosen our feature vector, we now want to build a discriminative model. So, we have to model $p(x|y)$. But if we have, say, a vocabulary of $50000$ words, then $x ∈ \{0, 1\}^{50000}$ ($x$ is a $50000$-dimensional vector of $0$ ’s and $1$ ’s), and if we were to model $x$ explicitly with a multinomial distribution over the $2^{50000}$ possible outcomes, then we’d end up with $(2^{50000} −1)$ parameter vectors. This is clearly too many parameters.
</div>
<p/>
<div>
To model $p(x|y)$, we will therefore make a very strong assumption. We will assume that the $x_i$ ’s are conditionally independent given $y$. This assumption is called the <b>Naive Bayes (NB) assumption</b>, and the resulting algorithm is called the <b>Naive Bayes classifier</b>. (Note that this is not the same as saying that $x_i$ ’s are independent, which would have been written $p(x_i) = p(x_i|x_j)$; rather, we are only assuming that $x_i$ ’s are conditionally independent given $y$.)
</div>

we now have:

$$ \begin{align*}
p(x_1,...,x_{50000}|y) &= p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)···p(x_{50000}|y,x_1,...,x_{49999}) \\
                         &= p(x_1|y)p(x_2|y)p(x_3|y)···p(x_{50000}|y) \\
                         &= \prod_{i=1}^{n} p(x_i|y)
\end{align*} $$

<div>
Our model is parameterized by $\phi_i|_{y=1} = p(x_i = 1|y = 1)$, $\phi_i|_{y=0} = p(x_i = 1|y = 0)$, and $\phi_y = p(y = 1)$. As usual, given a training set $\{(x^{(i)}, y^{(i)}); i = 1, ... , m\}$, we can write down the joint likelihood of the data:
</div>

$$ L(\phi_y, \phi_i|_{y=0}, \phi_i|_{y=1}) = \prod_{i=1}^{m} p(x^{(i)}, y^{(i)}) $$

<div>
Maximizing this with respect to $\phi_y$, $\phi_i|_{y=0}$ and $\phi_i|_{y=1}$ gives the maximum likelihood estimates:
</div>

$$ \begin{align*}
\phi_j|_{y=1} &= \frac{\sum_{i=1}^{m} 1\{x_j^{(i)}=1, y^{(i)}=1\}}{\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \\
\\
\phi_j|_{y=0} &= \frac{\sum_{i=1}^{m} 1\{x_j^{(i)}=1, y^{(i)}=0\}}{\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \\
\\
       \phi_y &= \frac{\sum_{i=1}^{m} 1\{y^{(i)}=1\}}{m}
\end{align*} $$

Having fit all these parameters, to make a prediction on a new example with features $x$, we then simply calculate

$$ \begin{align*}
p(y = 1|x) &= \frac{p(x|y = 1)p(y = 1)}{p(x)} \\
           &= \frac{(\prod_{i=1}^{n}p(x_i|y=1))p(y = 1)}{(\prod_{i=1}^{n}p(x_i|y=1))p(y = 1) + (\prod_{i=1}^{n}p(x_i|y=0))p(y = 0)}
\end{align*} $$

and pick whichever class has the higher posterior probability.

<div>
Lastly, we note that while we have developed the Naive Bayes algorithm mainly for the case of problems where the features $x_i$ are binary-valued, the generalization to where $x_i$ can take values in $\{1, 2 ,..., k_i\}$ is straightforward. Here, we would simply model $p(x_i|y)$ as multinomial rather than as Bernoulli. Indeed, even if some original input attribute were continuous valued, it is quite common to discretize it - that is, turn it into a small set of discrete values by mapping continuous valued to different range intervals - and apply Naive Bayes. When the original, continuous-valued attributes are not well-modeled by a multivariate normal distribution, discretizing the features and using Naive Bayes (instead of GDA) will often result in a better classifier.
</div>

#### Laplace Smoothing

The Naive Bayes algorithm as we have described it will work fairly well for many problems, but there is a simple change that makes it work much better, especially for text classification. Lets briefly discuss a problem with the algorithm in its current form, and then talk about how we can fix it.

<div>
Consider the text has a word that you never seen it before. Assuming that word was the 35000th in the dictionary, Naive Bayes text classification therefore had picked its maximum likelihood estimates of the parameters $\phi_{35000}|_y$ to be
</div>

$$ \begin{align*}
\phi_{35000}|_{y=1} &= \frac{\sum_{i=1}^{m} 1\{x_{35000}^{(i)} = 1, y^{(i)}=1\}}{\sum_{i=1}^{m} 1\{y^{(i)}=1\}} = 0\\
\\
\phi_{35000}|_{y=0} &= \frac{\sum_{i=1}^{m} 1\{x_{35000}^{(i)} = 1, y^{(i)}=0\}}{\sum_{i=1}^{m} 1\{y^{(i)}=0\}} = 0 \\
\end{align*} $$

I.e., because the 35000th word never seen before in any training examples, it thinks the probability of seeing it is zero. Hence, when trying to classify the text containing that word, it calculates the class posterior probabilities, and obtains

$$ \begin{align*}
p(y = 1|x) &= \frac{(\prod_{i=1}^{n}p(x_i|y=1))p(y = 1)}{(\prod_{i=1}^{n}p(x_i|y=1))p(y = 1) + (\prod_{i=1}^{n}p(x_i|y=0))p(y = 0)} \\
&= \frac{0}{0}
\end{align*} $$

<div>
This is because each of the terms $\prod_{i=1}^{n} p(x_i|y)$ includes a term $p(x_{35000}|y) = 0$ that is multiplied into it. Hence, our algorithm obtains $0/0$, and doesn’t know how to make a prediction.
</div>

Stating the problem more broadly, it is statistically a bad idea to estimate the probability of some event to be zero just because you haven’t seen it before in your finite training set. Take the problem of estimating the mean of a multinomial random variable $z$ taking values in {$1, . . . , k$}. We can parameterize our multinomial with $\phi_i = p(z = i)$. Given a set of $m$ independent observations {$z^{(1)} , . . . , z^{(m)}$}, the maximum likelihood estimates are given by

$$ \phi_j = \frac{\sum_{i=1}^{m} 1\{z^{(i)}=j\}}{m} $$

As we saw previously, if we were to use these maximum likelihood estimates, then some of the $\phi_j$ ’s might end up as zero, which was a problem. To avoid this, we can use **Laplace Smoothing**, which replaces the above estimate with

$$ \phi_j = \frac{1 + \sum_{i=1}^{m} 1\{z^{(i)}=j\}}{k + m} $$

Here, we’ve added $1$ to the numerator, and $k$ to the denominator. Note that $\sum_{j=1}^{k} \phi_j = 1$ still holds, which is a desirable property since the $\phi_j$ ’s are estimates for probabilities that we know must sum to $1$. Also, $\phi_j \neq 0$ for all values of $j$, solving our problem of probabilities being estimated as zero. Under certain (arguably quite strong) conditions, it can be shown that the Laplace smoothing actually gives the optimal estimator of the $\phi_j$ ’s.

Returning to our Naive Bayes classifier, with Laplace smoothing, we therefore obtain the following estimates of the parameters:

$$ \begin{align*}
\phi_j|_{y=1} &= \frac{1 + \sum_{i=1}^{m} 1\{x_j^{(i)}=1, y^{(i)}=1\}}{2 + \sum_{i=1}^{m} 1\{y^{(i)}=1\}} \\
\\
\phi_j|_{y=0} &= \frac{1 + \sum_{i=1}^{m} 1\{x_j^{(i)}=1, y^{(i)}=0\}}{2 + \sum_{i=1}^{m} 1\{y^{(i)}=0\}} \\
\end{align*} $$

In practice, it usually doesn’t matter much whether we apply Laplace smoothing to $\phi_y$ or not, since we will typically have a fair fraction each of different classification, so $\phi_y$ will be a reasonable estimate of $p(y = 1)$ and will be quite far from $0$ anyway.

#### Event models for text classification

To close off our discussion of generative learning algorithms, lets talk about one more model that is specifically for text classification. While Naive Bayes as we’ve presented it will work well for many classification problems, for text classification, there is a related model that does even better.

<div>
In the specific context of text classification, Naive Bayes as presented uses the what’s called the <b>multivariate Bernoulli event model</b>. In this model, we assumed that the way a text is generated is that first it is randomly determined (according to the class priors $p(y)$) its classification. Then, runs through the dictionary, deciding whether to include each word $i$ in that text independently and according to the probabilities $p(x_i = 1|y) = \phi_i|_y$. Thus, the probability of a text was given by $p(y) \prod_{i=1}^{n} p(x_i|y)$.
</div>
<p/>
<div>
Here’s a different model, called the <b>multinomial event model</b>. To describe this model, we will use a different notation and set of features for representing texts. We let $x_i$ denote the identity of the $i$-th word in the text. Thus, $x_i$ is now an integer taking values in $\{1, . . . , |V|\}$, where $|V|$ is the size of our vocabulary (dictionary). A text of $n$ words is now represented by a vector $(x_1 , x_2 , . . . , x_n)$ of length $n$; note that $n$ can vary for different texts.
</div>
<p/>
<div>
In the multinomial event model, we assume that the way a text is generated is via a random process in which classification is first determined (according to $p(y)$) as before. Then, the text by first generating $x_1$ from some multinomial distribution over words. Next, the second word $x_2$ is chosen independently of $x_1$ but from the same multinomial distribution, and similarly for $x_3$, $x_4$, and so on, until all $n$ words of the text have been generated. Thus, the overall probability of a text is given by $p(y) \prod_{i=1}^{n} p(x_i|y)$. Note that this formula looks like the one we had earlier for the probability of a text under the multivariate Bernoulli event model, but that the terms in the formula now mean very different things. In particular $x_i|y$ is now a multinomial, rather than a Bernoulli distribution.
</div>
<p/>
<div>
The parameters for our new model are $\phi_y = p(y)$ as before, $\phi_i|_{y=1} = p(x_j = i|y = 1)$ (for any $j$) and $\phi_i|_{y=0} = p(x_j = i|{y = 0})$. Note that we have assumed that $p(x_j|y)$ is the same for all values of $j$ (i.e., that the distribution according to which a word is generated does not depend on its position $j$ within the text).
</div>

If we are given a training set {$(x^{(i)}, y^{(i)}); i = 1, . . . , m$} where $x^{(i)} = (x_1^{(i)} , x_2^{(i)} , . . . , x_{n_i}^{(i)})$ (here, $n_i$ is the number of words in the $i$-training example), the likelihood of the data is given by

$$ \begin{align*}
L(\phi_y, \phi_i|_{y=0}, \phi_i|_{y=1}) &= \prod_{i=1}^{m} p(x^{(i)}, y^{(i)}) \\
&= \prod_{i=1}^{m} \bigg(\prod_{j=1}^{n_i} p(x_j^{(i)}|y^{(i)}; \phi_i|_{y=0}, \phi_i|_{y=1})\bigg) p(y^{(i)};\phi_y)
\end{align*} $$

Maximizing this yields the maximum likelihood estimates of the parameters

$$ \begin{align*}
\phi_k|_{y=1} &= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k, y^{(i)}=1\}}{\sum_{i=1}^{m} 1\{y^{(i)}=1\} n_i} \\
\\
\phi_k|_{y=0} &= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k, y^{(i)}=0\}}{\sum_{i=1}^{m} 1\{y^{(i)}=0\} n_i} \\
\\
       \phi_y &= \frac{\sum_{i=1}^{m} 1\{y^{(i)}=1\}}{m}
\end{align*} $$

<div>
If we were to apply Laplace smoothing (which needed in practice for good performance) when estimating $\phi_k|_{y=0}$ and $\phi_k|_{y=1}$, we add $1$ to the numerators and $|V|$ to the denominators, and obtain:
</div>

$$ \begin{align*}
\phi_k|_{y=1} &= \frac{1 + \sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k, y^{(i)}=1\}}{|V| + \sum_{i=1}^{m} 1\{y^{(i)}=1\} n_i} \\
\\
\phi_k|_{y=0} &= \frac{1 + \sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)}=k, y^{(i)}=0\}}{|V| + \sum_{i=1}^{m} 1\{y^{(i)}=0\} n_i} \\
\end{align*} $$

While not necessarily the very best classification algorithm, the Naive Bayes classifier often works surprisingly well. It is often also a very good “first thing to try,” given its simplicity and ease of implementation.
